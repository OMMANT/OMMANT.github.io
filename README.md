출처: https://arxiv.org/pdf/1906.12320.pdf

<figure>
<img src="https://ommant.com/wp-content/uploads/2021/08/PointFlow_fig1.jpg" alt="우리 모델은 연속적인 정규화 흐름을 통해 현실적인 포인트 클라우드 이전의 단순에서 샘플링된 포인트를 변환합니다. 변환 비디오는 프로젝트 웹사이트에서 볼 수 있습니다.">
<figcaption>그림 1: 우리 모델은 연속적인 정규화 흐름을 통해 현실적인 포인트 클라우드 이전의 단순에서 샘플링된 포인트를 변환합니다. 변환 비디오는 프로젝트 <a href="https://www.guandaoyang.com/PointFlow">웹 사이트</a>에서 볼 수 있습니다.</figcaption>

</figure>

## 개요
3D 포인트 클라우드가 여러 그래픽 표현에 선택됨으로써 물체를 선명하고 정확히 표현하는 포인트 클라우드를 합성하거나 재구성하는 기능이 중요해지고 있습니다. 최근 딥러닝 모델은 포인트 클라우드를 분류하는데 성공했지만, 포인트 클라우드를 생성하는 것은 어려운 일입니다. 이 논문은 3차원 포인트 클라우드 분포를 모델링하여 생성하기 위한 확률적 프레임워크를 제안합니다. 구체적으로 2단계 계층 구조로 나눠 학습합니다. 첫째는 모양의 분포, 둘째는 주어진 모양에 대한 점의 분포를 학습하는 것입니다. 이 공식을 사용하면 모양을 샘플링하고 그 모양에서 임의의 수의 점을 샘플링할 수 있습니다. PointFlow라는 생성 모델은 지속적인 정규화 흐름으로 분포의 각 수준을 학습합니다. *흐름 정규화의 가역성*은 훈련 중 가능성의 계산을 가능하게 하고 *변형 추론 프레임워크*에서 모델을 훈련할 수 있게 합니다. 경험적으로 우리는 PointFlow가 포인트 클라우드 생성에서 최첨단 성능을 달성함을 보여줍니다. 우리는 또한 우리 모델이 비지도 방식으로 포인트 클라우드를 충실하게 재구성하고 유용한 표현을 학습할 수 있음을 보여줍니다. 코드는 이 [주소](https://github.com/stevenygd/PointFlow)에서 확인할 수 있습니다.

## 1. Introduction
포인트 클라우드는 복셀 그리드보다 훨씬 더 높은 해상도를 캡처할 수 있고 메쉬와 같은 보다 정교한 표현을 위한 디딤돌이 되기 때문에 3D 표현으로써 인기를 얻고 있습니다. 이전 포인트 클라우드보다 더 나은 것을 제공함으로써 포인트 클라우드의 생성 모델 학습은 재구성이나 초해상도와 같은 다양한 포인트 클라우드 합성에 유용합니다. 그러나 포인트 클라우드 생성의 주요 장애물은 포인트 클라우드 공안의 복잡성입니다. 의자를 표현하는 포인트 클라우드는 분표는 의자 표면에 해당하는 분포로부터의 샘플로, 의자 자체는 의자 모양의 분포로부터의 샘플로 생각할 수 있다. 그 결과로 이런 방식에 따른 의자를 생성하기 위해 우리는 분포들로부터 제대로 탐색되지 않은 분포를 특성화해야한다.
이 논문에서는 우리는 3D 포인트 클라우드를 위한 분포들로부터 분포를 학습하는 원리적 생성 모델인 PointFlow를 제안합니다. 분포들은 모양이 주어진 점의 분포이며 분포는 모양의 분포입니다. 우리의 핵심 통찰력은 모양의 점 분포를 직접 매개변수화하는 대신 이 분포를 이전 분포(3D 가우시안)에서 3D 점의 가역 매개변수화 변환으로 모델링한다는 것입니다. 직관적으로 이 모델에서 주어진 모양에 대한 점을 생성하는 것은 일반적인 가우시안 사전에서 점을 샘플링한 다음 그림 1과 같이 매개변수화된 변환에 따라 대상 모양의 새 위치로 이동하는 것을 포함합니다. 이 공식에서 주어진 모양은 단순히 그런 변환을 매개변수화하는 변수이고 카테고리는 단순히 이 변수의 분포입니다. 흥미롭게도, 우리는 이 분포를 이전 분포의 변형으로 표현한다는 것이 더 표현적인 모양 모델로 이어진다는 것을 발견했습니다. 특히, 우리는 두 종류의 변환을 모델링하기 위해 최근에 제안된 연속 정규화 흐름 프레임워크를 사용합니다.
이 매개변수화는 몇 가지 이점을 제공합니다. 이러한 변환의 가역성을 통해 우리는 표본을 추출할 수 있을 뿐만아니라 확률 밀도도 추정할 수 있습니다. **순서대로 확률 밀도를 추정하는 기능은 우리가 포인트 클라우드 훈련 세트의 로그 가능성에 대한 변동 하한을 최대화하는 변형 추론 프레임워크를 사용하여 원칙적인 방식으로 이러한 모델을 훈련할 수 있게 해줍니다.** 훈련을 위한 이 확률적 프레임워크를 사용하면 GAN을 훈련하거나 두 점 세트 간의 차이를 측정하기 위한 좋은 거리 측정항목을 손으로 만드는 복잡성을 피할 수 있습니다. 실험에 따르면 PointFlow는 포인트 클라우드의 이전 최첨단 생성 모델보다 성능이 뛰어나며 포인트 클라우드 재구성 및 비지도 기능 학습에서 놀라운 결과를 달성합니다.

## 2. 관련 연구
### 포인트 클라우드에 대한 딥러닝
딥러닝은 분류(Classification), 세분화(Segmentation), 임계점 샘플링(Critial point sampling)을 포함한 다양한 포인트 클라우드 판별 작업에서 성능을 개선하기 위해 도입되었습니다. 최근, 오토-인코딩, 싱글뷰 3D 재구성(Reconstruction), 스테레오 재구성(Reconstruction), 포인트 클라우드 완성과 같은 포인트 클라우드 합성 작업에서 상당한 진전이 이루어졌습니다. 많은 포인트 클라우드 합성 작업은 기존 생성 모델을 쉽게 적용할 수 있도록 분포에서 N(N은 미리 정의됨) 포인트를 샘플링하여 포인트 분포를 N x 3행렬로 변환합니다. 예를 들어, VAE(Variational Auto-Encoder) 및 포인트 클라우드 생성에 적대적 자동 인코더(AAE)를 적용합니다. 원시 데이터 공간(raw data space)과 사전 훈련된 오토-인코더의 잠재 공간 모두에서 포인트 클라우드에 대한 생성적 적대 네트워크(GAN)을 탐색합니다. 위의 방법에서 오토-인코더는 **Chamfer distance(CD)** 또는 **Earth mover's distance(EMD)**와 같은 두 점 집합 간의 거리를 측정하는 휴리스틱 손실 함수로 훈련됩니다. 이산 점 분포가 있는 자동 회귀 모델을 적용하여 모양당 고정된 수의 점을 사용해 한 번에 하나의 점을 생성합니다.

그러나 포인트 클라우드를 고정 차원 행렬로 처리하는 데는 몇 가지 단점이 있습니다. 첫째, 모델은 고정된 수의 점을 생성하도록 제한합니다. 특정 모양에 대해 다 많은 포인트를 얻으려면 별도의 업샘플링 모델이 필요합니다. 둘째, 최적이 아닌 매개변수 효율성으로 이어질 수 있는 포인트 세트의 순열 분별 속성을 무시합니다. 휴리스틱 집합 거리는 VAE(Variational Auto-Encoder)/AAE(Adverserial Auto-Encoder)의 원래 확률적 해석을 재구성 목표로 사용할 때 더 이상 적용할 수 없도록 만들기 때문에 생성적 모델링 관점에서 이상적인 목표와 거리가 멉니다. 게다게 정확한 EMD는 계산 속도가 느린 반면 근사값은 편향되거나 노이즈가 심한 그라디언트를 유발할 수 있습니다. CD는 주변 포인트 분포 모드에 지나치게 집중된 포인트 클라우드를 잘못 선호하는 것으로 나타났습니다.

일부 최근 연구에서는 캐스캐이드로 혹은 작은 디코더들의 혼합으로 구성된 복잡한 디코더를 도입하여 하나(또는 혼합)의 2D 균일 분포를 대상 지점 분포에 매핑하여 고정된 수를 사용하는 단점을 극복하는 것을 소개합니다. 그러나 여전히 확률적 보장이 없는 휴리스틱 집합 거리(HSD)에 의존합니다. 또한 그 방법은 각 모양에 대한 점의 분포만 학습하고 모양의 분포는 학습하지 않습니다. WGAN 손실과 EMD의 변형을 결합하는 "샌드위치" 재구성을 목표를 제안합니다. Achlioptas의 유사한 형태 분포를 배우기 위해 잠재 공간에서 또 다른 GAN을 훈련합니다. 대조적으로, 우리의 방법은 로그 가능성에 대한 변동 하한을 최대화하여 종단 간 훈련을 하고, 다단계 훈련이 필요하지 않으며, GAN기반 방법에서 흔히 볼 수 있는 불안정 문제가 없습니다.

### 생성 모델
적대적 생성 네트워크, 변형 오토-인코더, 자동 회귀 모델 및 흐름 기반 모델을 포함하여 심층 생성 모델의 몇 가지 인기있는 프레임워크가 있습니다. 특히 흐름 기반 모델과 자동 회귀 모델은 모두 정확한 가능성 평가를 수행할 수 있지만 흐름 기반 모델은 샘플링하는 데 훨씬 더 효율적입니다. 흐름 기반 모델은 이미지 생성, 비디오 생성 및 음성 합성과 같은 다양한 생성 작업에 성공적으로 적용되었습니다. 또한 최근 흐름 기반 모델과 GAN, 자동 회귀 모델 및 VAE와 같은 다른 생성 모델과 결합하는 연구가 있습니다. 

기존 대부분의 생성 모델은 고정 차원 변수의 분포를 학습하는 것을 목표로 합니다. 집합중의 집합으로 구성된 데이터를 분포중의 분포를 학습하는 것은 여전히 부족합니다. Edwards와 Storkey는 집합중의 집합을 참조하는 Neural Statistician이라는 계층적인 VAE를 제안합니다. 대부분 각 세트에 몇 개의 샘플만 있는 소수의 경우에 관심이 있습니다. 또한 집합을 분류하거나 주어진 집합에서 새 샘플을 생성하는 데 중점을 둡니다. 우리의 방법은 이러한 작업에도 적용 가능하지만, 우리의 초점은 집합의 분포를 배우고 새로운 집합을 생성하는 것입니다(우리의 경우 포인트 클라우드). 또한, 우리 모델은 재구성 가능성과 사전 가능성을 모두 모델링할 때 흐름을 정규화하는 사용 덕분에 로그 가능성에 대해 더 엄격한 하한을 사용합니다.

## 3. 개요
$`\chi=\{X_i\}^N_{i=1}`$



## 4. 배경지식
### 4.1 연속 정규화 흐름
**정규화 흐름**은 초기의 알려진 분포를 더 복잡한 것으로 변환하는 일련의 가역적인 매핑이다. 공식적으로, $f_1, ...,  f_n$을 분포 $P(y)$를 갖는 잠재 변수 $y$에 적용하려는 일련의 가역적인 변환으로 가정하자. $x = f_{n} \bullet f_{n-1} \bullet ... \bullet f_{1}(y)$은 결과 변수이다. 그러면 결과 변수에 대한 확률 밀도는 변수의 변화에 대한 식으로 나타낼 수 있다.
$$\log{P(x)}=\log{P(y)} - \sum_{k=1}^{n}{\log{\left |\det{\partial{f_{k}} \over {\partial{y_{k-1}}}}\right |}}, \tag{1}$$
여기서 y는 역 흐름을 사용해 x로부터 계산할 수 있다: $y = f_{1}^{-1} \bullet ... \bullet f_{n}^{-1}(x)$. 실제로, $f_1, ..., f_{n}$은 보통 자코비안 행렬 $\left | \det{{\partial{f_{k}} \over {\partial{y_{k-1}}}}} \right |$의 행렬식을 계산하기 쉽도록 만드는 구조를 사용해 신경망으로 인스턴스화됩니다. 정규화 흐름은 연속 시간 dynamic ${{\partial{y(t)}}\over{\partial {t}}}=f(y(t), t)$를 사용하여 변환 f를 정의함으로써 이산 시퀀스에서 연속 변환으로 일반화되었습니다. 여기서 f는 무제한 아키텍처를 갖는 신경망입니다. 시작 시간에 사전 분포 P(y)가 있는 P(x)에 대한 연속 정규화 흐름 모델은 다음과 같이 작성할 수 있습니다:
$$x=y(t_0)+\int_{t_0}^{t_1}{f(y(t), t)dt}\quad y(t_{0})\sim P(y)$$
$$\log P(x) = \log P(y(t_{0})) - \int_{t_0}^{t_1}{Tr \left( \partial f \over \partial y(t)\right)}dt \tag 2$$
이고 $y(t_0)$는 역 흐름 $y(t_0)=x+int_{t_1}^{t_0}{f(y(t), t)dt}$로부터 계산될 수 있습니다. 블랙박스 상미분 방정식(ODE) 솔버를 적용하여 연속 정규화 흐름의 출력과 입력 복사를 추정할 수 있습니다.
### 4.2 변형 오토-인코더 (VAE)
생성 모델을 구축할 확률 변수 $X$가 있다고 가정하자. 변형 오토-인코더(VAE)는 $X$의 관찰 데이트 셋으로부터 $P(X)$를 학습할 수 있는 프레임워크이다. VAE는 사전 분포 $P_{\psi}(z)$가 있는 잠재 변수 $z$와 $z$가 주어진 $X$의 분포를 캡처하는 디코더 $P_{\theta}(X|z)$를 통해 데이터 분포를 모델링합니다. 훈련중에 추가로 추론 모델(또는 인코더) $Q_{\psi}(z|X)$를 학습합니다. 인코더와 디코더는 관찰의 로그 가능성에 대한 하한을 최대화하도록 공동으로 훈련됩니다.
$$\log P_\theta(X) \geq \log P_\theta(X) - D_{KL}(Q_\phi(z|X) || P_\theta(z|X)) 
\\\qquad\qquad\qquad\qquad=\mathbb{E}_{Q_{\phi}(z|x)}[\log P_{\theta}(X|z)] - D_{KL}(Q_{\phi}(z|X) || P_{\psi}(z))
\\\qquad\qquad\quad\triangleq \mathcal{L}(X;\phi,\psi,\theta)\quad\tag{3}$$
이것은 evidence lower bound(ELBO)라고 불리기도 합니다. ELBO를 음의 재구성 오차(첫 번째 항)와 잠재 공간 정규화기(두 번째 항)의 합으로 해석할 수 있습니다.

\(x\)
