# 적대적 생성 네트워크를 위한 스펙트럼 정규화(Spectral Normalization for Generative Adverserial Networks)

[출처](https://arxiv.org/pdf/1802.05957.pdf): Spectral Normalization for Generative Adversarial Networks
(Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida)

## 개요
적대적 생성 네트워크를 연구하는 데 있어 문제점 중 하나는 학습이 불안정하다는 것이다. 이 논문에서는 판독기 훈련을 안정화하기 위한 **"스펙트럼 정규화(Spectral Normalization)"** 라는 새로운 가중치 정규화 기법을 제안한다. 이 새로운 정규화 기법은 이미 사용되는 방법에 통합시키기 계산적으로(Computationally) 가볍고 쉽다. 이 **"스펙트럼 정규화"** 의 효율성을 `CIFAR 10`, `STL-10`, `ILSVRC2012` 데이터 셋에서 시험해보았다. 그 결과 **"스펙트럼 정규화된 적대적 생성모델(SN-GANs)"** 은 이전 안정화 기법을 이용한 훈련에 비해 더 낫거나 같은 품질의 이미지를 생성할 수 있음을 실험적으로 확인하였다. Chainer(Tokui외, 2015)을 이용한 코드, 생성된 이미지, 사전-훈련된 모델은 [여기](https://github.com/pfnet-research/sngan_projection)에서 확인할 수 있다.

## 1. 소개
***적대적 생성 네트워크(GANs)***(Goodfellow 외, 2014)은 최근 몇 년 동안 생성 모델의 프레임워크로 상당한 성공을 거두었고 수많은 종류의 작업과 데이터 셋(Radford 외, 2016; Salimans 외. 2016; Ho & Ermon, 2016; Li 외, 2017)에 적용되었습니다. 간단히 말하면, GAN은 주어진 목표 분포를 흉내내는 모델 분포를 생성하기 위한 프레임워크이며 모델 분포를 생성하는 생성기(generator)와 모델 분포를 구별하는 판독기로 구성된다. 적대적 생성 네트워크의 개념은 연속하여 모델 분포와 판독기를 차례로 훈련하고 각 단계에서 최상의 판독기로 측정된 목표 분포와 모델 분포 사이의 차이를 줄이는 것을 목표로하는 것이다. 머신 러닝 커뮤니티에서 GAN은 고도로 구조화된 분포를 학습하는 능력뿐만 아니라 이론적으로 흥미로운 측면으로 관심을 끌어왔다. 예를 들어, (Nowozin 외, 2016; Uehara 외, 2016; Mohamed & Laksh-minarayanan, 2017)은 판독기의 훈련이 좋은 판별기(discriminator)의 훈련은 모델 분포와 목표 사이의 좋은 밀도 비 추정기(estimator)를 훈련하는 것과 마찬가지라는 것을 밝혀냈다. 이것은 밀도 함수에 대한 직접적인 지식 없이도 변형 최적화(variational optimization)를 수행하는 데 사용할 수 있는 *암시적인 모델(implicit models)* (Mohamed & Lakshminarayanan, 2017; Tran 외, 2017)의 방법의 문을 여는 관점이다.

GAN 훈련에 있어 지속되는 문제는 판별기(discriminator) 성능 제어이다. 고차원 공간에서 판별기에의한 밀도비 추정은 훈련중에 종종 부정확하고 불안정하며 생성기 네트워크는 목표 분포의 다양한 구조를 학습하는데 실패한다. 심지어 모델 분포의 지원(support)와 목표 분포의 지원이 분리되어 있을 때 모델 분포와 목표 분포를 완벽하게 구별할 수 있는 판별기가 존재한다(Arjovsky & Bottou, 2017). 일단 그런 판별기가 이런 상황에 만들어지면, 생성기의 훈련은 완전히 멈춘다. 왜냐하면 입력에 대해 그렇게 만들어진 판별기의 미분값이 0으로 되기 때문이다. 이것은 우리가 판별기의 선택에 어떤 형태의 제한을 도입하도록 동기를 부여합니다. 

이 논문에서는 판별기 네트워크의 훈련을 안정화할 수 있는 *스펙트럼 정규화*라 불리는 새로운 가중치 정규화 방법을 제안합니다. 이 정규화는 아래와 같은 유리한 속성을 즐깁니다.
* Lipschitz 상수는 조정해야하는 유일한 하이퍼-파라미터이며 알고리즘은 만족스러운 성능을 위한 유일한 하이퍼-파라미터의 강한 조정을 요구 하지 않습니다.
* 실행은 간단하고 추가적인 컴퓨팅 비용은 적습니다.

사실, 우리의 정규화 방법은 유일한 하이퍼-파라미터인 Lipschitz 상수의 조절 없이도 잘 작동합니다. 이 연구에서 우리는 가중치 정규화(Salimans & Kingma, 2016), 가중치 조각(clipping)(Arjovsky 외, 2017), 경사도 벌칙(gradient penalty, Gulranjani 외, 2017)와 같은 다른 정규화 기법과 비교하여 GAN을 위한 스펙트럼 정규화의 효율성에대한 설명을 제공합니다. 우리는 또한 무료 정규화 기술(예: 배치 정규화, 가중치 감쇠 및 판별자에 대한 특징 일치)이 없는 경우 스펙트럼 정규화가 가중치 정규화 및 그래디언트 페널티보다 생성된 이미지의 완전한 품질을 개선할 수 있음을 보여줍니다.

## 2. 방법
이 섹션에서는 제안된 방법에 대한 이론적 토대를 마련할 것입니다. 입력 $x$로 아래 형식을 따르는 신경 네트워크로 만든 간단한 판별기를 생각합시다:
$$f(x, \theta) = W^{L+1}a_{L}(W^{L}(a_{L-1}(W^{L-1}(...\ a_{1}(W^{1}x)\ ...)))), \tag{1}$$
$\theta := \lbrace W^{1},\ ...\ , W^{L}, W^{L+1} \rbrace$ 은 학습 파라미터 셋이고, $W^{l} \in \mathbb{R}^{d_{l} \times d_{l-1}},\ W^{L+1} \in \mathbb{R}^{1 \times d_{L}}$을 만족하고, $a_l$은 요소별 비선형 활성화 함수(element-wise non-linear activation function)이다. 간단히 표현하기 위해 각 레이어마다의 편향항(bias term)은 생략하였습니다. 판별기의 최종 출력은 아래와 같습니다.
$$D(x, \theta) = \mathcal{A}(f(x, \theta)), \tag{2}$$
$\mathcal{A}$은 사용자의 측정하기로 선택한 거리의 발산에 해당하는 활성화 함수입니다. 표준적인 GAN의 공식은 아래와 같습니다.
$$\underset{G}{\min}\ {\underset{D}{\max}}\ V(G, D)$$
$G$의 최솟값과 $D$의 최댓값은 각각 생성기와 판별기 함수에 해당합니다. $V(G, D)$의 일반적인 형태는 다음과 같이 주어집니다. $E_{x\sim q_{data}} \lbrack \log{D(x)} \rbrack + E_{x' \sim p_{G}} \lbrack \log{(1-D(x'))} \rbrack$ 이 때, $\ q_{\mathrm{data}}$는 데이터 분포이고, $p_{G}$는 적대적 최소-최대 최적화를 통해 학습해야 하는 (모델)생성기 분포입니다. 이 식의 $D$에 사용된 활성화 함수 $\mathcal{A}$은 닫힌 구간 [0, 1]에서 연속 함수(예, 시그모이드 함수)이다. 고정된 생성기 $G$에 대해 $V(G, D)$의 최적의 판별기는 $D_{G}^{*}(x) := q_{\mathrm{data}}(x)/q_{\mathrm{data}}(x)+p_{G}(x))$을 따른다는 것은 알려져 있다.

머신러닝 커뮤니티는 최근 판별기가 선택되는 함수 공간이 GAN의 성능에 결정적으로 영향을 미친다는 것을 지적해왔다. 많은 연구(Uehara 외, 2016; Qi, 2017; Gulrajani 외, 2017)는 통계의 유계성(boundedness)을 보증하는 Lipschitz 연속성의 중요성을 지지했다. 예를 들어, 위의 표준 공식을 따르는 GAN의 최적 판별기 아래 공식을 따르고
$$D_{G}^{*}(x)={q_{\mathrm{data}}(x)\over q_{\mathrm{data}}(x)+p_{G}(x)} = \mathrm{sigmoid}(f^{*}(x)),\ \mathsf{where}\ f^{*}(x)=\log{q_{\mathrm{data}}(x)}-\log{p_{G}(x)}, \tag{3}$$
이것의 미분값은
$$\nabla_{x}{f^{*}(x)}={1 \over q_{\mathrm{data}}(x)} \nabla_{x}{q_{\mathrm{data}}(x)} - {1 \over p_{G}(x)} \nabla_{x}{p_{G}(x)} \tag{4}$$
무한할 수 있고 계산 불가능할 수 있다. 이것은 $f(x)$의 도함수에 약간의 규칙성 조건을 도입하도록 촉구한다. 

이 일련의 과정에서 특히 성공적인 연구는 (Qi, 2017; Arjovsky 외, 2017; Gulrajani 외, 2017)이다. 이 연구는 입력 $x$에 대한 규제항을 추가함으로써 판별기의 Lipschitz 상수를 제어하는 방법을 제안했다. 우리는 그들의 발자취를 따라 K-Lipschitz 연속함수 집합으로 부터 판별기 $D$를 칮으려 한다. 즉,
$$\underset{||f||_{\mathrm{Lip}} \leq K}{\mathrm{argmax}} V(G, D), \tag{5}$$
$||f||_{\mathrm{Lip}}$은 임의의 $x,\ x'$에 대해 $||f(x)-f(x')||/||x-x'|| \leq M$가 되는 최솟값 $M$을 의미하며 Norm은 $l_2$ Norm이다. 

입력 기반 규제는 상대적으로 샘플에 대해 쉬운 공식을 허락하는 반면, 약간의 경험적(heuristic) 수단 없이 생성기와 데이터 분포의 지원 외부 공간에 규제를 도입할 수 없다는 사실로 고통받는다. 이 논문에서 소개할 *스펙트럼 정규화* 라는 방법은 Yoshida & Miyato(2017)가 고안한 기술을 사용하여 가중치 행렬을 정규화하여 이 문제를 회피하는 것을 목표로 한다. 

### 2.1 스펙트럼 정규화
스펙트럼 정규화는 문자 그대로 각 층 $g: \boldsymbol{h}_{in} \mapsto \boldsymbol{h}_{out}$의 스펙트럼 노름(norm)을 제한하여 판별기 함수 $f$의 Lipschitz 상수를 제어한다. 정의에 따르면, Lipschitz 노름 $||g||_{\mathrm{Lip}}$은 $\mathrm{sup}_{\boldsymbol{h}} \sigma(\nabla g(\boldsymbol{h}))$와 같다. $\sigma (A)$은 행렬 $A$의 스펙트럼 노름이다.($A$의 $L_2$ 행렬 노름)
$$\sigma(A) := \underset{\boldsymbol{h}:\boldsymbol{h}\neq0}{\max}{||A \boldsymbol{h}||_{2} \over || \boldsymbol{h}||_{2}}= \underset{||\boldsymbol{h}||_{2} \leq 1}{\max} ||A \boldsymbol{h}||_{2} \tag{6}$$
이는 $A$의 가장 큰 특이값(singluar value)에 해당한다. 따라서, 선형층 $g(\boldsymbol{h}) = W\boldsymbol{h}$에 대해 노름은 $||g||_{\mathrm{Lip}}=\mathrm{sup}_{\boldsymbol{h}}\sigma(\nabla(\boldsymbol{h}))=\mathrm{sup}_{\boldsymbol{h}}\sigma(W)=\sigma(W)$으로 표현할 수 있다. 만약 활성화 함수 $||a_{l}||_{\mathrm{Lip}}$의 Lipschitz 노름이 1과 같다면<a href="#footnote1" id="1footnote">1</a>, 우리는 $||f||_{\mathrm{Lip}}$에 대한 아래의 경계를 관찰하기 위해 부등식 $||g_{1} \circ g_{2}||_{\mathrm{Lip}} \leq ||g_{1}||_{\mathrm{Lip}}\cdot ||g_{2}||_{\mathrm{Lip}}$을 사용할 수 있다.
$$||f||_{\mathrm{Lip}} \leq || ( \boldsymbol{h}_{L} \mapsto W^{L+1} \boldsymbol{h}_{L}||_{\mathrm{Lip}} \cdot ||a_{L}||_{\mathrm{Lip}} \cdot || ( \boldsymbol{h}_{L-1} \mapsto W^{L} \boldsymbol{h}_{L-1}||_{\mathrm{Lip}}\\
\cdots ||a_{1}||_{\mathrm{Lip}} \cdot || ( \boldsymbol{h}_{0} \mapsto W^{1} \boldsymbol{h}_{0} ) ||_{\mathrm{Lip}} = \prod_{l=1}^{L+1}{||(\boldsymbol{h}_{l-1} \mapsto W^{l} \boldsymbol{h}_{l-1} ||_{\mathrm{Lip}}} = \prod_{l=1}^{L+1}{\sigma(W^{l})}. \tag{7}$$
*스펙트럼 정규화* 는 가중치 행렬 $W$의 스펙트럼 노름을 정규화하여 Lipschitz 제약 $\sigma(W)=1$을 만족하도록 한다.
$$\bar{W}_{\mathrm{SN}}(W) := W / \sigma(W). \tag{8}$$
만약 식(8)을 사용하여 각각의 $W^{l}$을 정규화하면, 부등식(7)과 $||f||_{\mathrm{Lip}}$가 1 위의 경계로 지정되었음을 확인하기 위해 $\sigma \left (\bar{W}_{\mathrm{SN}}(W) \right ) = 1$이라는 사실이 마음에 들 수 있다.

여기서 스펙트럼 정규화와 Yoshida & Miyato(2017)가 제안한 스펙트럼 노름 "규제"와의 차이점을 강조하고싶다. 스펙트럼 정규화와는 달리 스펙트럼 노름 "규제"는 스펙트럼 노름에 목표 함수에 명백한 규제항을 추가함으로써 벌칙(penalty)을 가한다. 그 방법은 스펙트럼 노름을 지정된 값으로 '설정'하려는 시도를 하지 않으므로 근본적으로 우리의 방법과 다르다. 심지어 정규화된 비용 함수의 도함수를 재구성하고 목표 함수 (12)를 다시 작성할 때, 우리의 방법은 샘플 데이터 *종속* 정규화 함수로 비용 함수를 증가시킨다는 것을 볼 수 있다. 반면, 스펙트럼 노름 규제는 L2 규제와 라쏘(Lasso)처럼 비용함수에서 샘플 데이터 *독립* 규제를 부과한다.

### 2.2 스펙트럼 노름 $\sigma(W)$의 빠른 추정
앞에서 언급했듯 각 층의 판별기를 규제하기 위해 사용하는 스펙트럼 노름 $\sigma(W)$은 $W$의 가장 큰 특이값이다. 만약 순진하게 알고리즘의 각 단계에서 $\sigma(W)$를 계산하기 위해 특이값 분해를 적용한다면, 알고리즘은 계산적으로 무거워질 수 있다. 대신, 우리는 $\sigma(W)$를 추정하기 위해 거듭제곱 방법을 사용할 수 있다 (Golub & Van der Vorst, 2000; Yoshida & Miyato, 2017). 거듭제곱 방법을 사용하면, 우리는 스펙트럼 노름을 단순한 GAN의 전체 컴퓨팅 비용 대비 아주 적은 추가적인 컴퓨팅 시간을 들여 추정할 수 있다. 자세한 방법은 부록 A를 참조하고 실제 스펙트럼 정규화 알고리즘의 요약은 알고리즘 1을 보세요.

### 2.3 스펙트럼의 정규화된 가중치의 경사도 분석
$W_{ij}$에 대한 $\bar{W}_{\mathrm{SN}}(W)$의 경사도 <a href="#footnote2" id="2footnote">2</a>는 아래와 같다.
$${\partial \bar{W}_{\mathrm{SN}}(W) \over \partial W_{ij}} = {1 \over \sigma(W)}E_{ij}-{1 \over \sigma(W)^{2}}{\partial \sigma(W) \over \partial W_{ij}}W = {1 \over \sigma(W)}E_{ij} - {\left [ \boldsymbol{u_{1}v_{1}^{\mathrm{T}}} \right]_{ij} \over \sigma(W)^{2}}W \tag{9}$$
$$= {1 \over \sigma(W)} \left( E_{ij} - \left [ \boldsymbol{u_{1}v_{1}^{\mathrm{T}}}\right ]_{ij} \bar{W}_{\mathrm{SN}}\right), \tag{10}$$
$E_{ij}$는 $(i, j)$번 째 입력은 1이고 다른 모든 곳에서는 0인 행렬이고 $\boldsymbol{u_1},\ \boldsymbol{v_{1}}$은 각각 $W$의 첫 번째 왼쪽과 오른쪽 특이 벡터이다. 만약 $\boldsymbol h$가 $\bar{W}_{\mathrm{SN}}$에 의해 변형되는 네트워크 내 은닉층이라면, 판별기 $D$의 가중치 $W$에 대한 미니배치에 대해 계산된 $V(G, D)$의 미분은 아래 식과 같다.
$${\partial V(G, D) \over \partial W} = {1 \over \sigma(W)} \left( \hat{\mathrm{E}} \left[ \boldsymbol{\delta h}^{\mathrm{T}}\right] - \left( \hat{\mathrm{E}} \left[\boldsymbol{\delta}^{\mathrm T}\bar{W}_{\mathrm{SN}}\boldsymbol{h}\right]\right) \boldsymbol{u_{1}v_{1}}^{\mathrm T} \right ) \tag{11}$$
$$={1 \over \sigma(W)}\left(\hat{\mathrm{E}} \left[ \boldsymbol{\delta h}^{\mathrm{T}}\right] - \lambda \boldsymbol{u_{1}v_{1}^{\mathrm{T}}} \right) \tag{12}$$
이 때, $\boldsymbol{\theta} := \left( \partial V(G, D) \over \partial( \bar{W}_{\mathrm{SN}}\boldsymbol{h}) \right)^{\mathrm T}$, $\lambda := \hat{\mathrm{E}}\lbrack \boldsymbol{\delta}^{\mathrm{T}}(\bar{W}_{\mathrm{SN}}\boldsymbol{h}) \rbrack$, $\hat{\mathrm{E}}[\cdot]$은 미니-배치에서의 실증적인 기댓값을 표현한다. 어떤 $k \in \mathbb R$에 대하여 $\hat{\mathrm{E}} \left[ \boldsymbol{\delta h^{\mathrm T}} \right ] = k \boldsymbol{u_{1}v_{1}^{\mathrm T}}$일 때,  ${\partial V \over \partial W }= 0$을 만족한다.

식 (12)에 대해 논평을 하도록 하자. 첫 번째 항 $\bar{\mathrm{E}} \left[ \boldsymbol{\delta h^{\mathrm T}}\right]$은 정규화 없이 가중치의 미분과 같다. 이러한 관점에서 이 식의 두 번째 항은 *적응* 정규화 계수 $\lambda$로 첫 번째 특이 성분에 불이익을 주는 규제항으로 볼 수 있다. $\lambda$는 $\boldsymbol{\delta}$와 $\bar{W}_{\mathrm{SN}} \boldsymbol{h}$이 비슷한 방향을 가리킬 때 양수이고 이것은 $W$의 열공간으로부터 훈련 과정에서 특정 방향으로 집중되는 것을 방지한다. 다시 말해, 스펙트럼 정규화는 각 층의 변형으로부터 한 방향에서 민감해지는 것을 방지한다. 스펙트럼 정규화를 사용하여 모델에 대한 새로운 매개변수화를 고안할 수 있습니다. 즉, 레이어 맵을 두 개의 학습 가능한 개별 요소: 스펙트럼 정규화 맵과 스펙트럼 노름 상수로 분할할 수 있다. 결과적으로 이 매개변수화는 그 자체로 장점이 있으며 GAN의 성능을 향상시켰습니다. (부록 E를 참고)

## 3. 스펙트럼 정규화 v.s. 다른 규제 기법
Salimas & Kingma(2016)이 제안한 가중치 정규화는 가중치 행렬에서 각 열 벡터의 $l_2$ 노름을 정규화하는 방법이다. 수학적으로 이것은 가중치 정규화 $\bar{W}_{\mathrm{WN}}$에 의한 가중치를 필요하는 것과 동등하다.:
$$\sigma_{1}(\bar{W}_{\mathrm{WN}})^{2}+\sigma_{2}(\bar{W}_{\mathrm{WN}})^{2}+\cdots+(\bar{W}_{WN})^{2}=d_{o}, \mathsf{where}\ T=\min(d_{i},\ d_{o}), \tag{13}$$
이 때, $\sigma_{t}(A)$는 행렬 $A$의 $t$-번째 특이값이다. 따라서 이것은 스케일러까지는 특이값들의 제곱의 합이 1이 되도록 하는 Frobenius 표준화와 동일하다. 그러나 이러한 표준화 기법들은 우연히 행렬에 의도한 것 보다 더 강한 제약을 부과할 수 있다. 만약 $\bar{W}_{\mathrm{WN}}$이 $d_{i} \times d_{o}$차원의 가중치가 표준화된 행렬이라면, 고정된 단위 벡터 $\boldsymbol h$에 대한 노름 $||\bar{W}_{\mathrm{WN}} \boldsymbol{h}||$은 $t = 2,\ \cdots, T$에 대하여 $\sigma_{1}(\bar{W}_{\mathrm{WN}})=\sqrt{d_{o}}$이고 $\sigma_{t}(\bar{W}_{\mathrm{WN}})=0$일 때 $||\bar{W}_{\mathrm{WN}}\boldsymbol{h}||_{2}=\sqrt{d_{o}}$에서 최대화된다. 이는 $\bar{W}_{\mathrm{WN}}$가 1순위 임을 의미한다. Frobenius 정규화에 대해서도 비슷한 내용을 말할 수 있다(자세한 정보는 부록 참고). 이러한 $\bar{W}_{\mathrm{WN}}$을 사용하는 것은 목표로부터 모델 확률 분포를 구별하기 위해 단 하나의 특성만 사용하는 것과 같다. 가능한 많은 입력 노름을 유지하고 더 민감한 판별기를 만들기위해 $\bar{W}_{\mathrm{WN}}\boldsymbol{h}$의 노름을 크게 만들기를 희망한다. 그러나 가중치 표준화에 대해 이것은 순위를 낮추고 판별기에 사용되는 특징의 개수를 줄이는 대가를 치르게 된다. 따라서, 가중치 표준화와 생성기 분포와 목표 분포를 구분하기 위해 가능한 많은 특징을 이용하려는 우리의 욕망사이의 이득의 충돌이 존재한다. 많은 경우 전자(가중치 표준화)의 이익은 종종 다른 이익보다 우선하여 판별기가 사용하는 기능의 수를 무심코 감소하는 경우가 많습니다. 결과적으로 알고리즘은 선택된 소수의 기능에서만 대상 분포와 일치하는 다소 임의적인 모델 분포를 생성한다. 가중치 클리핑(weight clipping, Arjovsky 외, 2017)도 같은 함정을 겪고 있다.

반면, 우리의 스펙트럼 표준화는 그런 이익의 충돌에 고통받지 않는다. 선형 연산자의 Lipschitz 상수는 최대 특이값에 의해서만 결정된다는 것에 주목하라. 즉, 스펙트럼 노름은 랭크에 독립적이다. 따라서, 가중치 표준화와는 달리 스펙트럼 표준화는 매개변수 행렬이 로컬 1-Lipschitz 제약을 만족하는 가능한 많은 특징을 사용할 수 있다. 스펙트럼 표준화는 특이값의 수를 결정하는 데 있어 더 많은 자유를 준다.

Brock 외(2016)은 GAN의 훈련을 안정화하기 위해 각 가중치에 직교 규제(orthogonal regularization)를 도입하였다. 그들의 연구에서 Brock 외(2016)은 다음 항을 추가하여 적대적 목적 함수(adversarial objective function)를 보강하였다.
$$||W^{\mathrm{T}}W-I||^{2}_{F}. \tag{14}$$
이것이 스펙트럼 정규화와 같은 목적을 수행하는 것처럼 보이지만 직교 정규화는 모든 특이값을 1로 설정함으로써 스펙트럼에 대한 정보를 파괴하기 때문에 스펙트럼 정규화 수학적으로 제법 다르다. 반면 스펙트럼 정규화는 오직 최대값이 1이 되도록 하기 위해 스펙트럼의 크기만을 조정한다. 

Gulrajaniet 외 (2017)은 WGAN과 조합하여 그라디언트 패널티기법을 사용하였다. 그들의 작업에서 그들은 생성 분포에서 샘플 $\tilde{x}$를, 샘플 $\boldsymbol{x}$로 부터 데이터 분포를 보간하여 생성된 식 $\boldsymbol{\hat{x}} := \epsilon \tilde{x} + (1 - \epsilon) \boldsymbol{x}$의 점들의 이산 집합에 지역 1-Lipschitz 상수(예를 들어, $||\nabla_{\hat{x}}f||_{2}=1$)를 갖는 함수를 보상하는 규제기(regularizer)를 이용해 목적 함수를 증강함으로써 판별기에 $K$-Lipschitz 상수를 배치하였다. 이 다소 간단한 접근방법은 특징 공간의 유효 차원과 관련된 앞서 말한 문제로 고통받지 않지만, 현재 생성 분포의 지원에 크게 의존한다는 명백한 단점이 있다. 물론 생성적 분포와 그 지원은 훈련 과정에서 점차 바뀌어 이런 규제 효과를 불안정하게 만들 수 있다. 실제로 우리는 높은 학습률이 WGAN-GP의 성능을 불안정하게 만들 수 있음을 경험적으로 관찰했다. 반대로 우리의 스펙트럼 정규화는 함수 연산자 공간을 정규화하고 정규화의 효과는 배치 선택과 관련하여 더 안정적이다. 스펙트럼 정규화를 사용한 훈련은 공격적인 학습률로 쉽게 불안정화되지 않는다. 게다게 WGAN-GP는 단일 단계 거듭 반복을 이용한 스펙트럼 정규화보다 더 많은 컴퓨팅 비용을 요구한다. 왜냐하면, $||\nabla_{\hat{\boldsymbol x}}f||_{2}$의 계산은 하나의 온전한 순-, 역-전파 단계를 요구한다. 부록 섹션에서 우리는 같은 양의 업데이트를 위한 두 방식의 컴퓨팅 비용을 비교한다.

## 4. 실험
우리의 방식의 효율성을 평가하고 그 효율성의 이유를 밝혀내기 위해 CIFAR-10(Torralba 외, 2008)과 STL-10(Coates 외, 2011)에서 광범위한 비지도 이미지 생성 실험을 하고 다른 정규화 기법들과 비교하였다. 우리의 방법이 대규모 데이터 셋에 어떻게 작동하는지 알아보기 위해 ILSVRC2012 데이터 셋(ImageNet, Russakovsky 외, 2015)에서도 적용해보았다. 이 섹션은 다음과 같이 구성된다. 첫 째, 훈련하기 구조를 훈련하기 위해 사용된 목적 함수에대해 의논한 다음 실험에서 사용된 최적화 설정을 설명할 것이다. 그 다음, 훈련된 생성기로 생성된 이미지의 평가를 위해 이미지에서의 두 성능 지표를 설명할 것이다. 마지막으로, 우리는 CIFAR-10, STL-10, ImageNet에서의 결과를 요약할 것이다.

구별기와 생성기의 구조는 합성곱 신경망을 사용하였다. 또한 합성곱 가중치 $W \in \mathbb{R}^{d_{\mathrm{out}}\times d_{\in} \times h \times w}$에 대한 스펙트럼 노름에 대해 우리는 차원 $d_{\mathrm{out}} \times ( d_{\mathrm{in}}h\omega)$의 2차원 행렬로서의 연산자를 다뤘다<a id="3footnote" href="#footnote3">3</a>. 우리는 배치 정규화(Ioffe & Szegedy, 2015)를 사용하여 생성기의 매개변수를 훈련하였다. 구조에 대한 더 많은 정보는 부록 섹션내 <a href="#table_3">표 3</a>을 참조하라.

WGAN-GP 이외의 모든 방법에 대해 우리는 적대적 손실에 대해 아래와 같은 표준 목적 함수를 사용했다:
$$V(G, D) := \underset{x \sim q_{\mathrm{data}(\boldsymbol x)}}{\mathrm E}[\log D(\boldsymbol{x})] + \underset{\boldsymbol{z} \sim p(\boldsymbol{z})}{\mathrm E} [\log (1-D(G(\boldsymbol{z})))], \tag{15}$$
이 때, $\boldsymbol{z} \in \mathbb{R}^{d}_{z}$는 잠재 변수이고, $p(\boldsymbol{z})$는 표준 정규 분포 $\mathcal{N}(0,\ I)$이고, $G: \mathbb{R}^{d_{z}} \mapsto \mathbb{R}^{d_{0}}$는 결정 생성 함수이다. 모든 실험에 대해 $d_{z}$를 128로 설정하였다. $G$의 업데이트를 위해 우리는 Goodfellow 외(2014)와 Warde-Farley & Bengio(2017)이 사용한 것과 같은 $\mathrm{E}_{\boldsymbol{z} \sim p(\boldsymbol{z})} [ \log (D(G(\boldsymbol{z})))]$ Goodfellow 외(2014)가 제안한 대체 비용(alternate cost)을 사용하였다. 아래와 같은 이른바 힌지 손실을 이용해 알고리즘의 성능을 테스트하였다.
$$V_{D}(\hat{G}, D) = \underset{\boldsymbol{x} \sim q_{\mathrm{data}}(\boldsymbol{x})}{\mathrm E} \left[ \min\ (0,\ -1+D(\boldsymbol{x})) \right] + \underset{\boldsymbol{z}\sim p(\boldsymbol{z})}{\mathrm E} \left[ \min \left( 0,\ -1-D \left( \hat{G}(\boldsymbol{z}) \right) \right) \right] \tag{16}$$
$$V_{G}(G, \hat{D})=-\underset{\boldsymbol{z} \sim p(\boldsymbol{z})}{\mathrm E} \left[ \hat{D}(G(\boldsymbol{z}))\right] \tag{17}$$
은 각각 판별기와 생성기에 대해서이다. 이러한 목표 함수를 최적화하는 것은 이른바 KL-역발산:$KL[p_{g}||q_{\mathrm{data}}]$을 최소화시키는 것과 같다. 이런 손실 유형은 이미 Lim & Ye(2017); Tran 외(2017)에서 제안되고 사용되었다. 힌지 손실에 기반한 알고리즘은 inception score와 FID로 평가했을 때 좋은 성능을 보였다. 그라디언트 패널티가 있는 Wasserstein GAN(WGAN-GP, Gulrajani 외, 2017)에 대해 우리는 아래와 같은 목적 함수: $V(G, D) := \mathrm{E}_{\boldsymbol{x} \sim q_{\mathrm{data}}}[D(\boldsymbol{x})] - \mathrm{E}_{\boldsymbol{z} \sim p(\boldsymbol{z})}[D(G(\boldsymbol{z}))] - \lambda \mathrm{E}_{\hat{\boldsymbol{x}}\sim p_{\hat{\boldsymbol{x}}}}[(||\nabla_{\hat{\boldsymbol{x}}}D(\hat{\boldsymbol{x}})||_{2}-1)^{2}$를 사용하였다. 이 때, 규제항은 부록 섹션 D.4에서 소개하였다.

생성된 예시의 정량적 평가에 대해 *inception score*(Salimans 외, 2016)과 $\mathit{Fr\acute{e}chet\ inception\ distance}$(FID, Heusel 외, 2017)를 사용하였다. 각 점수에 대한 자세한 정보는 부록 B.1을 참고하라.

### 4.1 CIFAR 10과 STL-10에서의 결과
이 섹션에서는 훈련 중 스펙트럼 정규화(스펙트럼 정규화된 GAN의 약어로 SN-GAN을 사용)의 정확도와 옵티마이저의 하이퍼파라미터에 대한 알고리즘 성능 의존도를 보고한다. 또, 가중치 클리핑(Arjovsky 외, 2017), WGAN-GP(Gulrajani 외, 2017), 배치 정규화(BN, Ioffe & Szegedy, 2015), 층 정규화(LN, Ba 외, 2016), 가중치 정규화(WN, Salimans & Kingma, 2016) 그리고 직교 정규화(*orthonormal*, Brock 외, 2016)를 포함한 판별기 네트워크에 대한 다른 규제/정규화 기법에 대한 알고리즘의 성능 수준을 비교한다. 그라디언트 패널티 단독의 효율성을 평가하기 위해 GAN의 표준 적대 손실에 그라디언트 패널티 항을 적용하였다. 우리는 이 방법을 'GAN-GP'라 부르기로 한다. 가중치 클리핑의 경우 우리는 Arjovskey 외(2017) 방식을 따르고 각 층의 합성곱에 클리핑 상수 $c$를 0.01로 설정하였다. 그라디언트 패널티의 경우 Gulrajani 외(2017)이 제안한대로 $\lambda$를 10으로 설정하였다. *직교*의 경우 무작위로 선택된 직교 연산자로 $D$의 각 가중치를 초기화하고, Brock 외 (2016)에서 사용된 정규화 항으로 증가된 목적 함수로 GAN을 훈련하였다. 전체 비교 연구에서 가중치 정규화 방법과 배치 정규화 및 계층 정규화 방법에서 승수 매개변수(multiplier parameter) $\gamma$를 제외하였다. 이는 Lipschitz 조건을 위반하는 것을 방지하는 방법이다.  다른 승수 매개변수로 실험했을 때, 실제로 개선할 수 없었다. 

최적화를 위해 모든 실험에서 Kigma & Ba(2015)의 Adam optimizer를 사용했다. 우리는 6가지 설정을 시험하였다. (1) $n_{\mathrm{dis}}$, 생성기 업데이트 1회 당 판별기 업데이트 횟수, (2) 학습도(learning rate) $\alpha$와 Adam의 1차 및 2차 모멘텀 매개변수($\beta_{1},\ \beta_{2}$). 이 설정의 자세한 정보는 부록 섹션의 표 1를 참고하라. 이 6개 세팅 외에도, A, B, C는 이전 대표 연구에서 사용된 설정이다. 설정 D, E와 F의 목표는 더 공격적인 학습도로 수행된 알고리즘의 성능을 평가하기 위함이다. 판별기와 생성기에서 사용된 합성곱 신경망에 대한 자세한 내용은 부록 섹션의 표 3을 참조하라. GAN 생성기에 대한 업데이트 수는 달리 명시되지 않는 한 모든 실험에서 100,000이다. 

우선 스펙트럼 정규화 절차가 실제로 그 목적에 부합하는지 확인하기 위해 훈련 중 각 레이어의 스펙트럼 노름을 검사했다. C.1의 그림 9에서 볼 수 있듯이 이 층들의 스펙트럼 노름은 훈련 전반에 걸쳐 1-1.05 사이 영역의 값을 가졌다. 자세한 정보는 부록 C.1를 보라.

그림 1과 2에서 A-F 설정으로 각 방법의 inception score를 보여준다. 우리는 스펙트럼 정규화가 공격적인 학습률과 모멘텀 매개변수를 사용하여 상대적으로 강력하다는 것을 알 수 있다. WGAN-GP는 CIFAR-10 및 STL-10에서 모두 높은 학습률과 높은 모멘텀 매개변수에서 우수한 GAN을 학습하는 데 실패하였다. 직교 정규화는 STL-10의 설정 E에 대해 좋은 성능을 보이지 못했지만 최적 설정의 방법보다 약간 더 잘 수행되었다. 이런 결과는 우리의 방법이 훈련 설정의 변화와 관련하여 다른 방법보다 더 강력함을 시사한다. 또 CIFAR-10보다 다양한 예제로 구성된 STL-10에서 WGAN-GP와 스펙트럼 정규화에 비해 최적의 가중치 정규화 성능이 떨어졌다. 스펙트럼 정규화의 최고 점수는 CIFAR-10과 STL-10 모두 거의 다른 모든 방법보다 우수하다.

표 2에서 CIFAR-10 및 STL-10 데이터 셋에 대한 최적의 설정으로 다양한 방법의 inception score를 보여준다. 우리는 SN-GAN이 최적 설정에서 거의 모든 것보다 더 가은 성능을 보임을 알 수 있다. SN-Gan은 힌지 손실(17)에서 훨씬 더 나은 성능을 보였다 <a href="#footnote4" id="4footnote">4</a>. 반복 횟수가 같은 훈련의 경우 SN-GAN은 STL-10에 대한 직교 정규화 규제보다 뒤쳐졌다. 직교 정규화와 스펙트럼 정규화 간의 더 자세한 비교는 섹션 4.1.2를 참조하라.

그림 6에서는 WGAN-GP, 가중치 정규화 및 스펙트럼 정규화로 훈련된 생성기에 의해 생성된 이미지를 보여준다. SN-GAN은 생성된 이미지의 품질 측면에서 가중치 정규화를 사용한 GAN보다 일관되게 더 우수했다. 좀 더 정확히 말하면 앞서 3장에서 언급했듯이 스펙트럼 정규화에 의해 생성된 이미지 셋은 가중치 정규화에 의해 생성된 이미지보다 더 명확하고 다양하다. 우리는 또 WGAN-GP가 높은 학습률과 높은 모멘텀(D, E, F)으로 좋은 GAN을 훈련하는 데 실패했음을 알 수 있다. GAN-GP, 배치 정규화, 계층 정규화로 생성된 이미지는 부록 섹션의 그림 12에 나와있다.

표 2의 하단에 결과를 요약한 여러 벤치마크 방법 및 알고리즘을 비교했다. 또 Gulrajani 외 에서 사용된 ResNet 기반 GAN에 이 방법을 적용한 ResNet 기반 GAN의 성능도 시험하였다. 나열된 모든 방법은 최적화 방법과 모델 구조가 다 다르다. 자세한 네트워크 구조는 부록 섹션의 표 4와 표 5릂 참조하라. 알고리즘 구현은 성능면에서 거의 모든 이전 모델보다 더 나은 성능을 발휘할 수 있다. 알고리즘의 구현은 성능면에서 거의 모든 이전 모델보다 더 나은 성능을 발휘할 수 있다.

---
<div class="footnote">
<p id="footnote1"><a href="#1footnote">1</a> 예를 들어, ReLU(Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011)와 Leaky ReLU(Maas et al., 2013)가 조건을 만족하며, 많은 인기 활성화 함수가 K-Lipschitz 제약 조건을 만족합니다. 일부 미리 정의된 K에 대해서도 마찬가지입니다.</p>
<p id="footnote2"><a href="#2footnote">2 </a>사실, 스펙트럼이 다중도(multiplicity)를 가지고 있을 때, 우리는 여기서 하위 그라디언트를 볼 것이다. 그러나, 이런 일이 일어날 확률은 0(거의 확실함)이므로 그런 사건을 고려하지 않고 논의를 계속할 것이다.</p>
<p id="footnote3"><a href="#3footnote">3 </a>합성곱은 이산적으로 수행하기 때문에 스펙트럼 노름은 보폭과 패딩의 크기에 따라 달라진다. 그러나 정답은 미리 정의된 일부 K에 의해서만 달라진다.</p>
<p id="footnote4"><a href="#4footnote">4 </a></p>
</div>
